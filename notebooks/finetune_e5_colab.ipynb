{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ”¥ Fine-Tune E5-Large for Jassas Arabic Search\n",
        "\n",
        "This notebook fine-tunes `intfloat/multilingual-e5-large` on your Arabic government documents.\n",
        "\n",
        "**Requirements:**\n",
        "- Runtime â†’ Change runtime type â†’ T4 GPU\n",
        "- Your `training_pairs.jsonl` file\n",
        "\n",
        "**Time:** ~1-2 hours for 5000 training pairs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q sentence-transformers datasets accelerate\n",
        "\n",
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Upload Training Data\n",
        "\n",
        "Upload your `training_pairs.jsonl` file (generated locally with `generate_training_data.py`)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Upload your training_pairs.jsonl file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check file\n",
        "import json\n",
        "with open('training_pairs.jsonl', 'r') as f:\n",
        "    pairs = [json.loads(line) for line in f]\n",
        "print(f\"\\nâœ“ Loaded {len(pairs)} training pairs\")\n",
        "print(f\"\\nExample:\")\n",
        "print(f\"  Query: {pairs[0]['query']}\")\n",
        "print(f\"  Positive: {pairs[0]['positive'][:80]}...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Prepare Training Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from sentence_transformers import InputExample\n",
        "\n",
        "# Load data\n",
        "with open('training_pairs.jsonl', 'r') as f:\n",
        "    pairs = [json.loads(line) for line in f]\n",
        "\n",
        "# Shuffle and split\n",
        "random.shuffle(pairs)\n",
        "split_idx = int(len(pairs) * 0.9)\n",
        "train_pairs = pairs[:split_idx]\n",
        "eval_pairs = pairs[split_idx:]\n",
        "\n",
        "print(f\"Train: {len(train_pairs)}, Eval: {len(eval_pairs)}\")\n",
        "\n",
        "# Create training examples with E5 prefixes\n",
        "# E5 REQUIRES \"query: \" and \"passage: \" prefixes!\n",
        "all_positives = [p['positive'] for p in pairs]\n",
        "\n",
        "train_examples = []\n",
        "for pair in train_pairs:\n",
        "    query = f\"query: {pair['query']}\"\n",
        "    positive = f\"passage: {pair['positive']}\"\n",
        "    \n",
        "    # Random negative (in-batch negatives will help too)\n",
        "    neg_candidates = [p for p in all_positives if p != pair['positive']]\n",
        "    if neg_candidates:\n",
        "        negative = f\"passage: {random.choice(neg_candidates)}\"\n",
        "        train_examples.append(InputExample(texts=[query, positive, negative]))\n",
        "\n",
        "print(f\"\\nâœ“ Created {len(train_examples)} training triplets\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Load Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_NAME = 'intfloat/multilingual-e5-large'\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "print(f\"âœ“ Model loaded\")\n",
        "print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Configure Training"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import losses, evaluation\n",
        "\n",
        "# Training config\n",
        "BATCH_SIZE = 8      # Reduce to 4 if OOM\n",
        "EPOCHS = 3          # 2-5 epochs is typical\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_RATIO = 0.1\n",
        "OUTPUT_DIR = './jassas-e5-arabic'\n",
        "\n",
        "# DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "    train_examples,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Loss: MultipleNegativesRankingLoss (InfoNCE)\n",
        "# This is THE standard loss for embedding fine-tuning\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
        "\n",
        "# Evaluator\n",
        "eval_examples = [\n",
        "    InputExample(texts=[f\"query: {p['query']}\", f\"passage: {p['positive']}\"], label=1.0)\n",
        "    for p in eval_pairs[:100]\n",
        "]\n",
        "evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(\n",
        "    eval_examples, name='arabic-gov'\n",
        ")\n",
        "\n",
        "# Calculate warmup\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
        "\n",
        "print(f\"Training config:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Warmup steps: {warmup_steps}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Train! ğŸš€"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# This will take 1-2 hours depending on data size\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=evaluator,\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path=OUTPUT_DIR,\n",
        "    show_progress_bar=True,\n",
        "    evaluation_steps=500,\n",
        "    save_best_model=True,\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Training complete!\")\n",
        "print(f\"  Model saved to: {OUTPUT_DIR}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Test the Fine-Tuned Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load fine-tuned model\n",
        "finetuned = SentenceTransformer(OUTPUT_DIR)\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    \"ÙƒÙ… ØºØ±Ø§Ù…Ø© ØªØ£Ø®ÙŠØ± ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙˆØ§Ø²ØŸ\",\n",
        "    \"Ù…Ø§ Ù‡ÙŠ Ø±Ø³ÙˆÙ… ØªØ¬Ø¯ÙŠØ¯ Ø±Ø®ØµØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŸ\",\n",
        "    \"ÙƒÙŠÙ Ø£Ø³ØªØ®Ø±Ø¬ ØªØ£Ø´ÙŠØ±Ø© Ø®Ø±ÙˆØ¬ ÙˆØ¹ÙˆØ¯Ø©ØŸ\"\n",
        "]\n",
        "\n",
        "# Test documents (mix of relevant and irrelevant)\n",
        "test_docs = [\n",
        "    \"ØºØ±Ø§Ù…Ø© ØªØ£Ø®ÙŠØ± ØªØ¬Ø¯ÙŠØ¯ Ø¬ÙˆØ§Ø² Ø§Ù„Ø³ÙØ± ØªØ¨Ù„Øº 500 Ø±ÙŠØ§Ù„ Ø¹Ù† ÙƒÙ„ Ø³Ù†Ø© ØªØ£Ø®ÙŠØ±\",\n",
        "    \"Ø±Ø³ÙˆÙ… Ø¥ØµØ¯Ø§Ø± Ø¬ÙˆØ§Ø² Ø§Ù„Ø³ÙØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯ 300 Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠ\",\n",
        "    \"Ø±Ø³ÙˆÙ… ØªØ¬Ø¯ÙŠØ¯ Ø±Ø®ØµØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø© 40 Ø±ÙŠØ§Ù„ Ù„Ù…Ø¯Ø© 5 Ø³Ù†ÙˆØ§Øª\",\n",
        "    \"ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ£Ø´ÙŠØ±Ø© Ø§Ù„Ø®Ø±ÙˆØ¬ ÙˆØ§Ù„Ø¹ÙˆØ¯Ø© Ø¹Ø¨Ø± Ù…Ù†ØµØ© Ø£Ø¨Ø´Ø±\",\n",
        "    \"Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„ÙŠÙˆÙ… Ù…Ø´Ù…Ø³ ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶\",  # Irrelevant\n",
        "]\n",
        "\n",
        "# Encode with prefixes\n",
        "q_embeddings = finetuned.encode([f\"query: {q}\" for q in test_queries])\n",
        "d_embeddings = finetuned.encode([f\"passage: {d}\" for d in test_docs])\n",
        "\n",
        "# Calculate similarities\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SIMILARITY MATRIX (Query vs Documents)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, query in enumerate(test_queries):\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    scores = np.dot(q_embeddings[i], d_embeddings.T)\n",
        "    ranked = sorted(zip(test_docs, scores), key=lambda x: -x[1])\n",
        "    for doc, score in ranked[:3]:\n",
        "        print(f\"  [{score:.3f}] {doc[:60]}...\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Download Fine-Tuned Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the model folder\n",
        "!zip -r jassas-e5-arabic.zip jassas-e5-arabic/\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download('jassas-e5-arabic.zip')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NEXT STEPS:\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Unzip jassas-e5-arabic.zip to your project's models/ folder\")\n",
        "print(\"2. Update src/tokenizer/vector.py:\")\n",
        "print(\"   MODEL_NAME = 'models/jassas-e5-arabic'\")\n",
        "print(\"3. Rebuild index: ./jassas build\")\n",
        "print(\"4. Test: ./jassas search 'ØºØ±Ø§Ù…Ø© Ø§Ù„Ø¬ÙˆØ§Ø²'\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Push to HuggingFace Hub"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to HuggingFace\n",
        "from huggingface_hub import login\n",
        "login()  # Enter your token\n",
        "\n",
        "# Push model\n",
        "finetuned.save_to_hub(\n",
        "    repo_id=\"your-username/jassas-e5-arabic\",\n",
        "    private=True\n",
        ")\n",
        "print(\"âœ“ Pushed to HuggingFace Hub!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
